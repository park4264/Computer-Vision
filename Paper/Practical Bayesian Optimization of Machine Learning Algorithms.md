# **Practical Bayesian Optimization of Machine Learning Algorithms**






- Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian Optimization of Machine Learning Algorithms. In Advances in Neural Information Processing Systems (pp. 2951-2959). (https://arxiv.org/abs/1206.2944)






<br>

<br>

# 0. Abstract

- **Machine learning algorithms** frequently require careful tuning of
    - model hyperparameters, 
    - regularization terms, 
    - optimization parameters.


<br>


- This tuning is often a â—¼ï¸ â€œblack artâ€ â—¼ï¸

- Much more appealing is the idea of developing automatic approaches which can optimize the performance of a given learning algorithm to the task at hand.

- ì¢‹ì€ ì•„ì´ë””ì–´ëŠ” ì§ì ‘ ì†ìœ¼ë¡œ í•˜ëŠ” ê²ƒë³´ë‹¤ ì´ê²ƒì„ ìë™í™” í•˜ëŠ” ê²ƒì´ë‹¤.

<br>

- âœ¨ In this work, we consider the automatic tuning problem within the framework of Bayesian optimization. âœ¨ 
    - It is a learning algorithmâ€™s generalization performance is modeled as a sample from a Gaussian process (GP). 



- Here we show how the effects of the Gaussian process prior and the associated inference procedure can have a <u>large impact</u> on the success or failure of Bayesian optimization.







- We show that these proposed algorithms improve on previous automatic procedures and can "reach or surpass" human expert-level optimization on a diverse set of contemporary algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.

- Priorë¡œ ì„¤ì •í•œ GPëŠ” í° ì—­í• ì„ í•œë‹¤.

<br>

<br>

# 1. Introduction

- ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì€ íŒŒë¼ë¯¸í„°ê°€ ì—†ê¸°ê°€ í˜ë“¤ë‹¤

    - The properties of a regularizer
    - the hyperior of a generative model
    - the step size of a gradient-based optimization

- Learning procedures almost always require a set of high-level choices that significantly impact generalization performance.

- ì´ëŸ¬í•œ ì¡°ì ˆì¥ì¹˜ ìµœì†Œí•œìœ¼ë¡œ í•˜ëŠ”ê²Œ ì¢‹ë‹¤.

<br>

- ê³ ìˆ˜ì¤€ íŒŒë¼ë¯¸í„°ì˜ ìµœì í™” ë¬¸ì œë¥¼ ìë™í™”í•˜ëŠ” ë” ìœ ì—°í•œ ë°©ë²•ì€, ì´ëŸ¬í•œ íŠœë‹ì„ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë°˜ì˜í•˜ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” <u>ë¸”ë™ë°•ìŠ¤ í•¨ìˆ˜ì˜ ìµœì í™”ë¡œ ê°„ì£¼</u>í•˜ê³ , ì´ë¥¼ ìœ„í•´ ê°œë°œëœ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒ.

- ì´ëŸ¬í•œ ìµœì í™” ë¬¸ì œëŠ” í•™ìŠµ ì ˆì°¨ì˜ ì €ìˆ˜ì¤€ ëª©ì  í•¨ìˆ˜ì™€ëŠ” ë‹¤ë¥¸ íŠ¹ì§•ì„ ê°€ì§€ë©°, ì—¬ê¸°ì—ì„œ í•¨ìˆ˜ í‰ê°€ëŠ” ê¸°ë³¸ ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì™„ë£Œí•˜ëŠ” ê²ƒì„ í•„ìš”ë¡œ í•˜ë¯€ë¡œ ë§¤ìš° ë¹„ìš©ì´ ë§ì´ ë“ ë‹¤. 

    - ğŸ’¡ **ë² ì´ì§€ì•ˆ ìµœì í™”** ğŸ’¡

    - ë² ì´ì§€ì•ˆ ìµœì í™”ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” í•¨ìˆ˜ê°€ ê°€ìš°ì‹œì•ˆ í”„ë¡œì„¸ìŠ¤(GP)ì—ì„œ ìƒ˜í”Œë§ëœ ê²ƒìœ¼ë¡œ ê°€ì •í•˜ê³ , í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ì¡°ì •í•˜ë©´ì„œ ì¼ë°˜í™” ì„±ëŠ¥ì„ ì¸¡ì •.

<br>

- í•˜ì§€ë§Œ! 
- ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ë¥¸ ìµœì í™” ë¬¸ì œì™€ ë‹¬ë¦¬ ê° í•¨ìˆ˜ í‰ê°€ì— ê±¸ë¦¬ëŠ” ì‹œê°„ì´ ë‹¤ë¥´ê³ , ë¹„ìš© ê°œë…ì„ ìµœì í™” ì ˆì°¨ì— í¬í•¨ì‹œí‚¤ëŠ” ê²ƒì´ ë°”ëŒì§í•˜ë‹¤
- ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì´ìš©í•´ ë³´ë‹¤ ë¹ ë¥´ê²Œ ìµœì ì˜ í•´ê²°ì±…ì— ë„ë‹¬í•  ìˆ˜ ìˆëŠ” ë² ì´ì§€ì•ˆ ìµœì í™” ì ˆì°¨ë¥¼ ê°œë°œ



<br>







- ì´ ë…¼ë¬¸ì˜ ê¸°ì—¬ëŠ” ë‘ ê°€ì§€ 
    1. ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ ìœ„í•œ ì¢‹ì€ ë°©ë²•ë¡ ì„ ë„ì¶œ
        - ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ë² ì´ì§€ì•ˆ ìµœì í™” ë°©ë²•ë¡ ì„ ì œì•ˆí•˜ë©°, ì»¤ë„ ë§¤ê°œë³€ìˆ˜ì˜ ì™„ì „í•œ ë² ì´ì§€ì•ˆ ì²˜ë¦¬ê°€ ê²°ê³¼ì˜ íƒ„ë ¥ì„±ì— ëŒ€í•œ ì¤‘ìš”ì„±ì„ ê°•ì¡°
        - in contrast to the more standard procedure of optimizing hyperparameters (e.g. Bergstra et al. (2011)).

    2. ë¹„ìš©(cost) ê°œë…ì„ ì‹¤í—˜ì— ë°˜ì˜í•˜ëŠ” ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì„ ì œì‹œ
        - ë¹„ìš© ê°œë…ì„ ê³ ë ¤í•œ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•˜ê³ , ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì´ìš©í•´ ë” ë¹ ë¥¸ ìµœì ì˜ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ë„ ì œì‹œ


<br>

<br>


# 2. Bayesian Optimization with Gaussian Process Priors.





- We are interested in finding the minimum of a function $f(\mathbf{x})$ on some bounded set $\mathcal{X}$, which we will take to be a subset of $\mathbb{R}^D.$

- ë‹¤ë¥¸ Optimizationê³¼ ë‹¤ë¥¸ì : 
    - It constructs a probabilistic model for $f(\mathbf x)$ and then exploits this model to make decisions about where in $\mathcal{X}$ to evaluate the function.
    - â• Uncertainty




- **The essential philosophy**: 
    - to use all of the information available from previous evaluations of $f(\mathbf{x})$ 
    - and <u> not simply rely on local gradient and Hessian approximations.</u>

<br>


- ë§Œì•½ $f(\mathbf{x})$ê°€ í‰ê°€í•˜ê¸°ì— ì‹œê°„ì´ ë§ì´ ë“ ë‹¤ê³  í–ˆì„ ë•Œ (ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ê°™ì€ ê²½ìš°)ë„ ì¢‹ë‹¤.


- ë¨¼ì € General Bayesian optimization approachë¥¼ ê°„ë‹¨íˆ ë¦¬ë·°í•˜ì.














<br>

## 2.1 Gaussian Processes

- The **Gaussian process (GP)** is a convenient and powerful prior
distribution on functions

- which we will take here to be of the form $$f: \mathcal{X} \rightarrow \mathbb{R}.$$

- The GP is defined by the property that any finite set of $N$ points $$\{ \mathbf{x}\in \mathcal{X} \}^N_{n=1}$$ induces a multivariate Gaussian distribution on $\mathbb{R}^N$

- The $n$ th of these points is taken to be the function value $f(\mathbf{x}_n)$

- ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ íŠ¹ì§• ì¤‘ í•˜ë‚˜ì¸ ë§ˆì§„í™”(marginalization properties)ë¥¼ ì´ìš©í•˜ë©´ ì¡°ê±´ë¶€ ë° ì£¼ë³€ í™•ë¥ ì„ ê°„ë‹¨í•˜ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤    
 
- The support and properties of the resulting dis tribution on functions are determined by 
    - a mean function $m: \mathcal{X} \rightarrow \mathbb{R}$ and
    - a positive definite covariance function $K: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}$ 


## 2.2. Acquisition Functions for Bayesian Optimization

> "Acquisition Functions for Bayesian Optimization"ì€ ë² ì´ì§€ì•ˆ ìµœì í™”ì—ì„œ ì‚¬ìš©ë˜ëŠ” íš¨ìœ¨ì ì¸ ì‹¤í—˜ ì‹¤í–‰ ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ íšë“ í•¨ìˆ˜(acquisition function)ì— ëŒ€í•œ ë…¼ë¬¸ì…ë‹ˆë‹¤. <u>ë² ì´ì§€ì•ˆ ìµœì í™”ì—ì„œ íšë“ í•¨ìˆ˜ëŠ” í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘í•œ ë°ì´í„°ë¡œë¶€í„° ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìœ„ì¹˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.</u> ì´ ë…¼ë¬¸ì—ì„œëŠ” ì—¬ëŸ¬ ê°€ì§€ íšë“ í•¨ìˆ˜ê°€ ì†Œê°œë˜ë©°, íš¨ìœ¨ì ì¸ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì˜ êµ¬ì¶•ì„ ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ìˆ ê³¼ ë°©ë²•ì´ ì œì•ˆë©ë‹ˆë‹¤. íšë“ í•¨ìˆ˜ëŠ” ë² ì´ì§€ì•ˆ ìµœì í™”ì˜ ì„±ëŠ¥ì„ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ìš”ì†Œ ì¤‘ í•˜ë‚˜ì´ë¯€ë¡œ, ì´ ë…¼ë¬¸ì€ ë² ì´ì§€ì•ˆ ìµœì í™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ë§ì€ ì—°êµ¬ìì™€ ì—”ì§€ë‹ˆì–´ë“¤ì—ê²Œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

- We assume that the function $f(\mathbf{x})$ is drawn from a GP prior,
- and that our observations are of the form $\{ \mathbf{x}_n, y_n \}^N_{n=1}$, where $y_n \sim \mathcal{N}(f(\mathbf{x}_n, \nu))$
    - $\nu$ is 






