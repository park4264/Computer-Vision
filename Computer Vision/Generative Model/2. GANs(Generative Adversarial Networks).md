# **Computer Vision - Generative Model**
# **2. GANs (Generative Adversarial Networks)** 




ì¶œì²˜: https://wikidocs.net/146217

# 2-1. Background ğŸ“š

- GANì€ 2014ë…„, Ian Goodfellowì˜ "Generative Adversarial Network"ë¼ëŠ” ë…¼ë¬¸ì—ì„œ ì²˜ìŒ ì œì‹œ
- ì§§ì€ ì‹œê°„ë™ì•ˆ ìƒë‹¹í•œ ë°œì „ì„ ë³´ì˜€ë‹¤.


![img](./img/2-1.png)


</Br>


# 2-2. How GAN works?

![img](./img/2-2.png)


</Br>
</Br>


- ğŸ“– **GANì˜ ê¸°ë³¸ ì² í•™** : ë³µì¡í•œ ê³ ì°¨ì›ì˜ training distributionì—ì„œ samplingì„ í•˜ê³ ì‹¶ë‹¤.
- ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ , simple distribution(e.g. random noise) ë¥¼ ìƒ˜í”Œë§ í•´ì„œ, Training distribution ì„ ë”°ë¥´ëŠ” Transformationì„ ì‹œí‚¬ ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµí•˜ì! ëŠ” ì „ëµ

![img](./img/2-3.jpeg)





</Br>

## 2-2-1. Generator and Discriminator








- GANì—ëŠ” ìœ„ì¡°ì§€íë²”ì— í•´ë‹¹í•˜ëŠ” **Generator(G, ğŸ¥·ğŸ»)** ì™€ ê²½ì°°ì— í•´ë‹¹í•˜ëŠ” **Discriminator(D, ğŸ‘®ğŸ»â€â™‚ï¸)** ê°€ ì¡´ì¬

  - ğŸ¥·ğŸ» **Generator**ëŠ” real dataì˜ distributionì„ í•™ìŠµí•´ fake ë°ì´í„°ë¥¼ ë§Œë“œëŠ” ì¼ì„ í•©ë‹ˆë‹¤.
    - â†’ ìµœì¢…ì ìœ¼ë¡œ ì´ë¥¼ Discriminatorê°€ ìµœëŒ€í•œ <U>í—·ê°ˆë¦¬ê²Œ í•˜ëŠ” ê²ƒì„ ëª©í‘œ</U>ë¡œ í•©ë‹ˆë‹¤.

  - ğŸ‘®ğŸ»â€â™‚ï¸ **Discriminator**ëŠ” smapleì´ realdata(training)ì¸ì§€ ì•„ë‹Œì§€ë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.
    - â†’ ìµœì¢…ì ìœ¼ë¡œ Fake ì´ë¯¸ì§€ë¥¼ ìµœëŒ€í•œ <U>ì˜ íŒë³„í•˜ëŠ” ê²ƒ ì„ ëª©í‘œ</U>ë¡œ í•©ë‹ˆë‹¤.


</Br>

![img](./img/2-3.png)



</Br>


## 2-2-2. Adversarial learning



![img](./img/2-5.png)


</Br>




- $p_{data}$:  training dataì˜ distribution
- $p_g (G(z))$: Generatorê°€ ë§Œë“¤ì–´ë‚¸ ì´ë¯¸ì§€ì˜ ë¶„í¬



</Br>

- GANì˜ ê¸°ë³¸ êµ¬ì¡° ìš”ì•½
  - (initialë¡œ ê³ ì •ëœ) Gë¡œ ìƒì„±
  - Dë¡œ classify, ì—…ë°ì´íŠ¸
  - (Dë¥¼ constantë¡œ ë§Œë“¤ê³ ) G ì—…ë°ì´íŠ¸
  - (ì—…ë°ì´íŠ¸ ëœ ìƒíƒœë¡œ ê³ ì •ëœ ) Gë¡œ ìƒì„±
  - Dë¡œ classify
  - (Dë¥¼ constantë¡œ ë§Œë“¤ê³ ) G ì—…ë°ì´íŠ¸
  - ...
  - ë°˜ë³µ
  - ...
  - ì´ ê³¼ì •ì„ ë°˜ë³µí•˜ë‹¤ê°€ D(x) = 1/2, ì¦‰ discriminatorê°€ êµ¬ë¶„í•  ìˆ˜ ì—†ëŠ” ìƒíƒœê°€ ë¨. $p_g = p_{data}$


</Br>
</Br>


## 2-2-3. Objective function of GAN

$$\min_\theta \max_\phi V(G_\theta, D_\phi) = \mathbb{E}_{\mathbf{x} \sim p_{data}(\mathbf{x})}[\log D_\phi (\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_{z}(\mathbf{z})}[\log(1-D_\phi(G_\theta(\mathbf{z})))]$$


### 2-2-3_1. ğŸ‘®ğŸ»â€â™‚ï¸ Discriminatorì˜ Objective function

- DiscriminatorëŠ” classifier
  - ë§Œë“¤ì–´ë‚¸ ë°ì´í„° $x$ê°€ **ì‹¤ì œ ë°ì´í„°ë¼ë©´ 1**ì„ ë¦¬í„´
  - **fake imageë¼ë©´ 0**ì„ ë¦¬í„´

- $D(x)$: Discriminatorì˜ ë¦¬í„´ ê°’

- Discriminatorì˜ Objective functionì€ ì•„ë˜ì™€ ê°™ë‹¤:


</Br>

![img](./img/2-6.png)

</Br>

- Discriminatorì˜ objective functionì€ ìµœëŒ€ ê°’ì´ 0, ìµœì†Œ ê°’ì€ $-\infin$
- ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒ: í•­ìƒ ì œëŒ€ë¡œ ì˜ˆì¸¡í•˜ëŠ” discriminatorê°€ ë˜ëŠ” ê²ƒ, ì¦‰ í•­ìƒ ìµœëŒ€ê°’ì¸ 0ì˜ ì„±ëŠ¥ì„ ë³´ì´ê¸°ë¥¼ ì›í•œë‹¤.

- **gradient ascending**, ìµœëŒ€ ê¸°ìš¸ê¸°ë¥¼ í–¥í•´ í•™ìŠµë˜ì–´ì•¼ í•¨






</Br>

</Br>


### 2-2-3_2. ğŸ¥·ğŸ» Generatorì˜ Objective function

- GeneratorëŠ” ìê¸°ê°€ ë§Œë“  fake ì´ë¯¸ì§€ê°€ **ì–¼ë§ˆë‚˜ discriminatorë¥¼ í—·ê°ˆë¦¬ê²Œ í–ˆëŠ”ì§€ ì´ê²ƒë§Œì´ ì¤‘ìš”í•œ ì •ë³´**

- âœ¨ ê·¸ë˜ì„œ discriminator objective functionì˜ ë’·ë¶€ë¶„, fake ì´ë¯¸ì§€ì— ëŒ€í•œ ê²°ê³¼ë§Œ ì´ìš©í•˜ë©´ ëœë‹¤. âœ¨

![img](./img/2-7.png)


- Discriminatorì™€ ë§ˆì°¬ê°€ì§€ë¡œ ìµœì†Œ $-\infin$, ìµœëŒ€ 0ì˜ ê°’ì„ ê°€ì§€ëŠ” ë¬¸ì œ
- Discriminatorë¥¼ ì†ì´ëŠ” ê²ƒì´ ëª©í‘œì´ê¸° ë•Œë¬¸ì— **gradient descent**ë¥¼ ì‹œí–‰




</Br>

</Br>

## 2-2-4. Optimizing GAN

### 2-2-4_1. $p_g = p_{data}$ê°€ ì •ë§ global optimumì¸ê°€?

- GANì˜ ê¸°ë³¸ ê°€ì •: $p_g = p_{data}$ì¼ ë•Œë¥¼ optimum
- ì´ê²ƒì´ í•©ë‹¹í•œê°€?


</Br>

- ì–´ë–¤ Gì— ëŒ€í•´ì„œë“  optimalí•œ Discriminatorë¥¼ ê°–ê³  ìˆë‹¤ê³  ê°€ì •

- ì´ ë•Œ (ê³ ì •ëœ Gì— ëŒ€í•œ) Dì˜ ì„±ëŠ¥ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. optimal í•˜ë‹¤ë©´ 0.5ë¡œ ìˆ˜ë ´í•˜ê² ì£ .


$$D^*_G = \dfrac{p_{data}(x)}{p_{data}(x) + p_g (x)}$$

- ì´ optimal í•œ Dì— ëŒ€í•œ objective functionì€ ì•„ë˜ì™€ ê°™ì„ ê²ƒ

$$\max_D V(G, D) = \mathbb{E}_{\mathbf{x} \sim p_{data}(\mathbf{x})}[\log D (\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_{G}}[\log(1-D(\mathbf{x}))]$$


- ìœ„ì˜ ì‹ì—ì„œ ê¸°ëŒ“ê°’ì„ $x$ì— ëŒ€í•œ í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚´ì–´ í‘œì‹œ

$$V(G, D) = \int_x  p_{data}(x) \log D (\mathbf{x})  +  p_g(x) (1-D(G(\mathbf{x})))$$

- For all $(a,b) \in \mathbb{R^2}$, $y \rightarrow a \log(y) + b\log(1-y)$ëŠ” $a/(a+b)$ì— Maximumìœ¼ë¡œ ë„ë‹¬.




</Br>

</Br>

### 2-2-4_2. Jenson-Shannon Divergence (JSD)

- ìœ„ì—ì„œ êµ¬í•œ optimal D, $D_G^* = \dfrac{p_{data}(x)}{p_{data}(x) + p_g(x)}$ ë¥¼ objective functionì— ì§ì ‘ ë„£ì–´ë³´ì

$$\mathbb{E}_{\mathbf{x} \sim p_{data}(\mathbf{x})}[\log \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}] + \mathbb{E}_{\mathbf{z} \sim p_{G}}[\log(1-\frac{p_{data}(x)}{p_{data}(x) + p_g(x)})]$$

$$= \mathbb{E}_{\mathbf{x} \sim p_{data}(\mathbf{x})}[\log \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}] + \mathbb{E}_{\mathbf{z} \sim p_{G}}[\log(\frac{p_{g}(x)}{p_{data}(x) + p_g(x)})]$$

$$= \mathbb{E}_{\mathbf{x} \sim p_{data}(\mathbf{x})}[\log \frac{p_{data}(x)}{(p_{data}(x) + p_g(x))/2}] + \mathbb{E}_{\mathbf{z} \sim p_{G}}[\log(\frac{p_{g}(x)}{(p_{data}(x) + p_g(x))/2})] - \log 4$$

$$= D_{KL}[p_{data}, \frac{p_{data} + p_g}{2}] + D_{KL}[p_{g}, \frac{p_{data} + p_g}{2}] - \log 4$$

$$= 2 D_{JSD}[p_{data}, p_g]- \log 4$$



</Br>

</Br>

### 2-2-4_3. Optimizing GAN


![img](./img/2-8.png)


</Br>

</Br>


# 2-3. Limitation

## 2-3-1. Training instability

![img](./img/2-9.png)

## 2-3-1. Mode collapse


![img](./img/2-10.png)

</Br>

</Br>

---
# ğŸ‘¨ğŸ»â€ğŸ”¬ Further study 





![img](./img/2-11.png)


