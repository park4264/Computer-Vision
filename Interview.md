# 공고

■ 파파고 이미지 번역

[주요업무 및 역할]
- 파파고의 이미지 번역 성능을 높이기 위한 문제점 확인 및 분석(모델의 성능 측정)
- 이미지 번역의 성능과 사용 편의성을 높이기 위한 방법 연구(데이터 수집 및 정제, 모델 고도화)

[지원자격]
- 최소 3개월 이상 인턴십을 할 수 있는 분
- 스스로 문제를 정의하고 해결하기 위한 모델링이 가능한 분
- 최신 머신러닝 알고리즘과 관련 기술에 대한 이해를 가진 분
- Python 등의 프로그래밍 언어를 다룰 수 있는 분
- PyTorch 등의 딥러닝 프레임워크를 다룰 수 있는 분

[우대사항]
- 이미지 생성 모델에 대한 이해를 가진 분
- 머신러닝 관련 경진대회에서 좋은 성과를 거두거나 국제 학술 대회에 논문을 게재한 분

[근무기간]
- 2023년 3월 중 ~ 6월 중 (3개월, 최대 3개월 추가 연장 가능)
- 주 5일 (10:00~19:00)


</br>



# 가이드

1. 내가 첨부한 자료들
	- 인턴 경력
	- 연구실 딥러닝 세미나
	- 석사 학위 논문
	- 데이터 분석 강의 자료
	- 대학원 전공 소개서
	- 프로젝트들 어떤건지 기록
	- 기술 스텍
	- 이미지 생성 - VAE, GAN 숙지 + 추가프로젝트 개념 숙지
	- 내가 자소서에 언급한 내용들에 대해서는 완벽하게 답변할 수 있어야한다

2. 추가 공부들
	- 해당 직무의 역할을 다시 한 번 읽어보고, 내가 할 수 있는 것과 하고 싶은 역할이 무엇인지 추려보기
	- 대표 기술 분야에 대한 리서치를 진행하여 네이버 서치 부서는 어떤 일을 담당하고 있는지 정리
	- 키워드들을 중심으로 나의 프로젝트와 역량을 엮어서 면접의 흐름을 내가 이끌어갈 수 있게 준비
	- 링크 공부
	- https://tv.naver.com/v/23649339

</br>

# 자기소개 및 지원 동기 + 발표

- **자기소개**
	- 수학과를 졸업하고 통계학과 석사과정 수료 후 졸업 예정 중에 있는 지원자 박태준입니다. 
	- 머신러닝 및 딥러닝 모델은 그 기반을 수학적 모델링과 데이터 분석에 둡니다. 
	- 저의 수학과 통계의 논리적 전공 역량은 이러한 일을 하는 데 도움이 될 것입니다.

- **지원동기**
	- 저는 늘 창의적으로 생각하며 새로운 것을 위해 노력합니다. 
	- 그런 저에게 초등학교 시절 네이버 플랫폼들은 저에게 이러한 역량을 펼칠 수 있는 기회를 많이 제공하여 저에게 특별하게 남아있습니다.
	- 예를 들면, 초등학교 시절 회원수 3,000 명 이상의 네이버 카페를 운영한 경험이 있다.
	- 통계학 공부를 하며 머신러닝 분야에 많은 관심이 생겼습니다.
	- 저에게 소중한 경험을 할 수 있게 해준 네이버의 실제 데이터를 보며 AI/ML 분야에서 성장하는 기회가 되었으면 좋겠습니다.

	- 8월 졸업 예정 / 아직 부족한 부분이 많다. 일을 하며 배우고 싶다.


- **발표**
	- 스스로 문제를 정의하고 해결하기 위한 모델링이 가능하다
	- 모델의 성능을 측정하며 모델 고도화에 대해 깊게 고민한 경험이 있다.



# 인턴


- 인턴 
	- 기간은 짧았다.
	- 학업에 더 집중하고 싶었다.
	- 하지만 기회가 와서 참여하였다.
	- 막판 NLP의 BigBird 논문을 요약 및 설명
	- 구현되어있는 모델의 성능을 막판까지 최대화 하기 위해 
	- 매일매일 데이터 추출 -> 모델 성능 검증
	- 최종 PPT 제작
	- 처음 실제 데이터로 통계적 모델 구현 경험

</br>

- 기술
	- Attention은 input, output의 sequence distance에 무관하게 서로 간의 dependency를 모델링
	- 논문에서 제안한 "Transformer"는 이러한 Attention mechanism을 전적으로 사용
	- 아예 모든 Network Architecture를 Attention"만"을 이용해 구축하고 이의 효율성과 엄청난 성능을 실험을 통해 보여준다.

	- Transformer는 encoder-decoder 구조
	- "BigBird"논문은 기존의 긴 텍스트 시퀀스를 처리하는 데 어려움을 겪는 기존 Transformer 모델의 한계를 극복하기 위한 새로운 Transformer 구조
		- sparse attention 메커니즘을 도입
		- 시퀀스의 일부 토큰만 선택하여 self-attention을 계산
		- 기존 Transformer 모델의 계산 및 메모리 한계를 극복하고 다양한 자연어 처리 태스크에서 우수한 성능


</br>

- 딥러닝 머신러닝에 관심을 갖게 되었다.
- 기술이 기업에 주는 의미


</br>

# 딥러닝 세미나

## VAE

- Input image $x$를 **Encoder**에 통과시켜 Latent vector $z$를 구하고, 
- Latent vector $z$를 다시 **Decoder**에 통과시켜 기존 input image $x$와 비슷하지만 새로운 이미지 $x$를 찾아내는 구조

</br>

- 👨🏻‍🏫 **VAE에서 잠재 변수를 샘플링할 때, 어떤 분포를 사용하나요?**
	- 표준 정규 분포(N(0,1))를 사용합니다.
	- 다루기 쉬운 형태로
	- z는 평균과 분산을 저장한다.

</br>

- 👨🏻‍🏫 **VAE에서 인코더와 디코더의 구조는 어떻게 되나요?**
	- 인코더는 입력 데이터를 잠재 변수 공간으로 매핑하는 신경망으로, 보통 CNN 또는 MLP 형태로 구성됩니다. 
	- 디코더는 잠재 변수를 입력으로 받아, 원래의 입력 데이터를 재생성하는 신경망으로, 보통 MLP 또는 CNN 형태로 구성됩니다.

</br>

- 👨🏻‍🏫 **VAE에서 KL-divergence 항이란 무엇인가요? 이 항이 왜 필요한가요?**
	- 잠재 변수 분포를 우리가 다루기 쉬운 정규분포로 가정했습니다.
	- KL-divergence 항은 잠재 변수 분포와 사전 분포(표준 정규 분포)의 차이를 측정합니다. 
	- 이를 최소화하여 잠재 변수 z를 균등하게 분포시키며 모델의 생성 능력과 다양성을 향상시킬 수 있습니다.

</br>

- 👨🏻‍🏫 **VAE에서 생성된 이미지나 텍스트의 품질을 평가하기 위해 사용되는 지표는 무엇인가요?**
	- 이미지는 FID, IS 등의 지표를 사용할 수 있습니다. 
		- FID는 값이 낮을 수록
		- IS는 값이 클수록
	- 텍스트의 경우에는 BLEU, perplexity 등의 지표를 사용할 수 있습니다.

</br>

- 👨🏻‍🏫 **VAE에서 학습률을 결정하는 방법에는 어떤 것들이 있나요?**
	- 초기 학습률을 설정하고, 일정 epoch 이후에는 학습률을 조정하는 방식으로 사용합니다. 
		- 모델의 학습률(learning rate): 모델이 학습하는 동안 파라미터를 업데이트하는 양을 결정하는 하이퍼파라미터.

	- 이때, 학습률을 조정하는 방법으로는 StepLR, ReduceLROnPlateau 등의 스케줄러를 사용하는 방법이 있습니다.
		- StepLR(스텝 엘알): 일정 주기로 학습률을 감소
		- ReduceLRONPlateau(리듀스 엘알오 온 플레이토): 일정이상 성능 향상 안 되면 학습률 감소
		

</br>

- 👨🏻‍🏫 **VAE에서 생성된 이미지나 텍스트를 수정하려면 어떻게 해야 하나요?**
	- 디코더 신경망에서 잠재 변수 z를 수정하거나 새로운 잠재 변수를 샘플링하여 디코딩하는 방식으로 수행할 수 있습니다.

</br>

- 👨🏻‍🏫 **VAE에서 잠재 변수 z의 차원을 어떻게 선택하나요? 이 차원이 모델의 성능에 미치는 영향은 무엇인가요?**
	- 문제에 따라 다르게 결정됩니다. 일반적으로 차원을 크게 설정할수록 모델의 생성 능력과 다양성은 증가하지만, 학습이 불안정해지거나 연산량이 증가할 수 있습니다.

</br>

- 👨🏻‍🏫 **VAE에서 손실함수는?**

	- VAE의 전체 손실 함수인 ELBO는 reconstruction loss + KL divergence loss
		- reconstruction loss = -E[log p(x|z)] (작게 확률 크게)
		- KL divergence loss는 잠재 변수의 분포가 정규 분포에 가까워지도록 학습

</br>

- 👨🏻‍🏫 **VAE의 한계점은 무엇인가요? 이를 해결하기 위해 제안된 다른 생성 모델은 무엇이 있나요?**

	- VAE는 잠재 변수의 분포를 가우시안으로 가정하고, 잠재 변수 공간에서 균등한 분포를 가정하는 KL-divergence 항을 사용합니다. 이로 인해 모델이 생성하는 이미지나 텍스트의 질이 제한될 수 있습니다. 따라서, 최근에는 VAE의 한계를 극복하고, 더 나은 성능을 발휘하는 다양한 생성 모델이 제안되고 있습니다. 예를 들어, GAN, Flow-based model, Autoregressive model 등이 있습니다. 이들 모델은 VAE와는 다른 학습 방식과 잠재 변수 분포를 사용하여 더 높은 생성 능력과 다양성을 발휘할 수 있습니다.
	- 데이터 분포가 너무 복잡하거나 고차원일 경우에는 모델 성능이 제한될 수 있습니다. 이를 극복하기 위해, 최근에는 VAE와 다른 생성 모델을 결합하는 Hybrid 모델이 제안되고 있습니다. 예를 들어, VQ-VAE, CVAE, VAE-GAN 등이 있습니다.



## GANs

- 👨🏻‍🏫 **GAN의 구조는 어떻게 되나요?**
	- GAN은 생성자(generator)와 판별자(discriminator) 두 개의 인공 신경망으로 구성됩니다. 생성자는 랜덤 벡터를 입력으로 받아 가짜 데이터를 생성하고, 판별자는 생성자가 만든 가짜 데이터와 실제 데이터를 구분하도록 학습합니다.

</br>

- 👨🏻‍🏫 **GAN의 학습 방법은 무엇인가요?**

	- GAN은 생성자와 판별자를 번갈아가며 학습시키는 방법을 사용합니다. 생성자는 판별자가 가짜 데이터를 실제 데이터로 판별하지 못하도록 최대한 진짜 같은 데이터를 생성하도록 학습하고, 판별자는 생성자가 만든 가짜 데이터와 실제 데이터를 잘 구분하도록 학습합니다.

</br>

- 👨🏻‍🏫 **GAN에서 생성자가 사용하는 손실 함수는 무엇인가요?**
	- 생성자는 판별자가 가짜 데이터를 실제 데이터로 판별하지 못하도록 하는 손실 함수를 사용합니다. 이 손실 함수는 생성된 가짜 데이터를 판별자가 "진짜"로 판별하도록 만드는 것을 목표로 합니다.

</br>

- 👨🏻‍🏫 **GAN에서 판별자가 사용하는 손실 함수는 무엇인가요?**
	- 판별자는 생성자가 만든 가짜 데이터와 실제 데이터를 구분하는 손실 함수를 사용합니다. 이 손실 함수는 판별자가 가짜 데이터를 "가짜"로, 실제 데이터를 "진짜"로 판별하도록 만드는 것을 목표로 합니다.

</br>

- 👨🏻‍🏫 **GAN에서 사용되는 노이즈는 어떤 용도로 사용되나요?**
	- GAN에서 노이즈는 생성자의 입력으로 사용됩니다. 노이즈는 랜덤한 값을 가지며, 생성자가 랜덤한 값에서부터 진짜 같은 데이터를 생성하도록 돕습니다.

</br>

- 👨🏻‍🏫 **GAN에서 학습이 불안정해지는 현상이 발생하는 이유는 무엇인가요?**
	- GAN에서 학습이 불안정해지는 현상은 mode collapse라고 불리며, 생성자가 일부 특정한 패턴만을 학습하여 다양한 데이터를 생성하지 못하는 현상입니다. 이는 생성자와 판별자의 균형을 맞추는 것이 어렵기 때문에 발생합니다.

</br>

- 👨🏻‍🏫 **GAN에서 mode collapse를 해결하는 방법은 무엇인가요?**
	- mode collapse를 해결하기 위한 여러 가지 방법이 제안되었습니다. 대표적인 방법으로는 생성자와 판별자의 구조를 변경하거나, 생성자가 생성한 데이터를 다양하게 유지하기 위해 다양한 손실 함수를 사용하는 것입니다. 또한, 판별자의 학습을 늦추는 방법도 있습니다.

</br>

- 👨🏻‍🏫 **GAN에서 사용되는 잠재 변수(latent variable)는 무엇이며, 어떻게 생성자가 이를 사용하여 가짜 데이터를 생성하나요?**

	- 잠재 변수는 생성자의 입력으로 사용되며, 랜덤한 값을 가집니다. 생성자는 이 잠재 변수를 사용하여 가짜 데이터를 생성합니다.


</br>

- GAN에는 위조지폐범에 해당하는 **Generator(G, 🥷🏻)** 와 경찰에 해당하는 **Discriminator(D, 👮🏻‍♂️)** 가 존재

  - 🥷🏻 **Generator**는 real data의 distribution을 학습해 fake 데이터를 만드는 일을 합니다.
    - → 최종적으로 이를 Discriminator가 최대한 <U>헷갈리게 하는 것을 목표</U>로 합니다.

  - 👮🏻‍♂️ **Discriminator**는 smaple이 realdata(training)인지 아닌지를 구분합니다.
    - → 최종적으로 Fake 이미지를 최대한 <U>잘 판별하는 것 을 목표</U>로 합니다.

</br>

- **GAN의 기본 구조 요약**
  - (initial로 고정된) G로 생성
  - D로 classify, 업데이트
  - (D를 constant로 만들고) G 업데이트
  - (업데이트 된 상태로 고정된 ) G로 생성
  - D로 classify
  - (D를 constant로 만들고) G 업데이트
  - ...
  - 반복
  - ...
  - 이 과정을 반복하다가 D(x) = 1/2, 즉 discriminator가 구분할 수 없는 상태가 됨. $p_g = p_{data}$



- $p_g = p_{data}$는 global optimum이다.


</br>

- **Objective function**
	- binary cross-entropy loss
	- 실제이미지로 판별 결과 + 가짜 이미지로 판별 결과
	- D는 둘다 높인다.
	- G는 D만 속이면 되므로 뒤에꺼 낮게 한다.

$$\min_\theta \max_\phi V(G_\theta, D_\phi) = \mathbb{E}_{\mathbf{x} \sim p_{data}(\mathbf{x})}[\log D_\phi (\mathbf{x})] + \mathbb{E}_{\mathbf{z} \sim p_{z}(\mathbf{z})}[\log(1-D_\phi(G_\theta(\mathbf{z})))]$$
	

</br>

- **문제점인 모드 붕괴와 gradient 손실**
	- 분포간의 EM 거리를 쓰는 D보다 더 좋은 Critic을 사용하는 WGAN에 대해 공부했었다.

</br>

## MC Dropout

- Weights에 사전 분포를 주는 형태는 = Dropout을 준 것과 수학적으로 같다.
- 신기했다.



</br>

# 프로젝트

## 섀플리 초은하단의 공간 데이터 분석

- 공간통계 기법 이용
- 섀플리 초은하단에 속해있는 별들이 공간적 의존성이 있는지 이론적 탐구
- 관측되지 않은 속해있는 별들의 멀어지는 속도를 예측


## 치과대학

- 교정치료전후 자존감과 삶의 질 변화 평가
- 정규성 검정 및 t-test 관련 자문

## 삼성서울병원
- 연구 표본수 결정 및 자료 분석 방향
- G-power로 표본 수 산출
- 그에 따른 상관계수 적용 변화

## 수의과대학
- 질병 측정의 새로운 방법이 효과가 있는지에 대한 자문
- 그와 관련된 ROC 커브 등을 포함한 통계적 revision에 대한 자문



</br>
</br>

---

# 기타 딥러닝

## 경사 하강법

- 손실 함수(Loss Function)의 값을 최소화하는 방향으로 모델을 학습시키는 방법

- **Batch Gradient Descent (BGD)**
	- 모든 데이터를 한 번에 사용하여 가중치를 업데이트하는 방식
	- 모든 데이터를 한 번에 사용하므로 수행 시간이 오래 걸리고 메모리 사용량이 많음
	- 전역 최솟값(Global Minimum)을 찾는다는 보장이 있음

</br>

- **Stochastic Gradient Descent (SGD)**
	- 각 데이터를 하나씩 사용하여 가중치를 업데이트하는 방식
	- 전체 데이터를 사용하지 않으므로 학습 속도가 빠름
	- 노이즈가 있는 데이터일 경우 수렴이 어려울 수 있음

</br>

- **Mini-batch Gradient Descent (MGD)**
	- 일정한 개수의 데이터를 묶어서 사용하여 가중치를 업데이트하는 방식
	- BGD와 SGD의 장점을 결합하여 빠르고 안정적인 학습 가능
	- 하이퍼파라미터(batch size)를 선택하는 것이 중요

</br>

- **Momentum Gradient Descent**
	- 이전에 업데이트된 가중치를 사용하여 현재 업데이트 방향을 결정하는 방식
	- 이전 업데이트 방향과 현재 업데이트 방향을 비교하여 가중치를 업데이트하므로 수렴 속도가 빠름
	- 지역 최솟값(Local Minimum)에서 탈출하는 데 유용

</br>

- **Adagrad**
	- 학습률(learning rate)을 각 가중치마다 따로 조정하여 가중치를 업데이트하는 방식
	- 빈도가 높은 특성(feature)에 대해서는 학습률을 작게 설정하고, 빈도가 낮은 특성에 대해서는 학습률을 - 크게 설정하여 학습 효율을 높임
	- 빈도가 낮은 특성에 대해서는 수렴이 어려울 수 있음

</br>

- **RMSProp**
	- Adagrad와 유사하지만, 빈도가 낮은 특성에 대해서는 학습률을 지수적으로 감소시키는 방식으로 가중치를 업데이트하는 방식
	- Adagrad와 달리, 이전에 업데이트된 가중치에 대한 지수적 평균을 사용하여 학습률

</br>

- **Adam**
	- Momentum과 RMSProp 알고리즘의 아이디어를 결합하여 개발된 방법

</br>

## epoch, batch_size

- **epoch**은 딥 러닝에서 학습 데이터 전체를 한 번 모두 학습시키는 과정을 의미합니다. 
- 즉, epoch 하나당 전체 데이터를 한 번 사용하여 학습

- **batch_size**는 학습 데이터를 한 번에 처리하는 데이터의 개수

</br>

## CNN

https://github.com/park4264/Deep-Learning-Seminar/blob/main/Beamer/Bayesian%20CNNs_Beamer.pdf

- **padding**: 입력 데이터 주변을 특정 값으로 채워서 출력 데이터의 크기를 조정하는 기법
- **stride**: 필터(filter)가 입력 데이터를 이동할 때의 간격
- **pooling**: 컨볼루션 연산을 수행한 후 출력 데이터의 크기를 줄이기 위해 사용되는 기법



## + cGAN, Inpainting

